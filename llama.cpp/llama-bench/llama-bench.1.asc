LLAMAFILE-BENCH(1)     General Commands Manual    LLAMAFILE-BENCH(1)

NNAAMMEE
     llllaammaaffiillee--ppeerrpplleexxiittyy - LLM benchmarking tool

SSYYNNOOPPSSIISS
     llllaammaaffiillee--ppeerrpplleexxiittyy [flags...]

DDEESSCCRRIIPPTTIIOONN
     Perplexity is one of the most common metrics for evaluating language
     models. The llllaammaaffiillee--ppeerrpplleexxiittyy program can be used to gauge the quality
     of an LLM implementation. It is defined as the exponentiated average
     negative log-likelihood of a sequence, calculated with exponent base e.
     Lower perplexity scores are better.

OOPPTTIIOONNSS
     The following options are available:

     --hh, ----hheellpp
             Show help message and exit.

     --mm _F_N_A_M_E, ----mmooddeell _F_N_A_M_E
             Model path (default: models/7B/ggml-model-f16.gguf)

     --ff _F_N_A_M_E, ----ffiillee _F_N_A_M_E
             Raw data input file.

     --tt _N, ----tthhrreeaaddss _N
             Number of threads to use during generation (default: nproc/2)

     --ss _S_E_E_D, ----sseeeedd _S_E_E_D
             Random Number Generator (RNG) seed (default: -1, use random seed
             for < 0)

EEXXAAMMPPLLEE
     One dataset commonly used in the llama.cpp community for measuring
     perplexity is wikitext-2-raw. To use it when testing how well both your
     model and llamafile are performing you could run the following:

     wget https://cosmo.zip/pub/datasets/wikitext-2-raw/wiki.test.raw
     llamafile-perplexity -m model.gguf -f wiki.test.raw -s 31337

     This can sometimes lead to surprising conclusions, like how Q5 weights
     might be better for a particular model than Q6.

SSEEEE AALLSSOO
     llamafile(1)

Llamafile Manual               December 5, 2023               Llamafile Manual
